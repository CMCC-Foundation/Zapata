{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac849237-d87f-4c3e-b82a-61f84b5d935f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403cadab-f550-46c0-9b02-aad5bbadc6f4",
   "metadata": {},
   "source": [
    "\n",
    "# SST.X621\n",
    "\n",
    "Predictions of monthly mean T2M from SST and other fields\n",
    "Generalization to N fields\n",
    "Using SST, T2M, Z500, U850, V850\n",
    "Global Tropics \n",
    " \n",
    "Using climformer from library\n",
    "\n",
    "Version with optional comparison with dynamical models\n",
    "Version with verification standard\n",
    "Version with smoothing\n",
    "\n",
    "Based on PYTORCH\n",
    "\n",
    "* Duplicate of X619 plus comparison with dynamical forecasts\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4fc7ab-0bdc-4e50-99fa-cd872c474e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory for Data  /Users/antonionavarra\n",
      "Working Directory set to  /Users/antonionavarra/CMCC Dropbox/Antonio Navarra/AI\n",
      "\n",
      "\n",
      "Notebook operating in ENV /Users/antonionavarra/opt/anaconda3/envs/AIDEV/bin/python\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import math\n",
    "import pickle\n",
    "import tabulate as tab\n",
    "import datetime\n",
    "import time as tm\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections  import namedtuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as sc\n",
    "import scipy.signal as sig\n",
    "import numpy.linalg as lin\n",
    "from scipy import stats\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import numpy.polynomial.polynomial as poly\n",
    "\n",
    "# AI Imports\n",
    "from sklearn import datasets\t\t# To Get iris dataset\n",
    "from sklearn import svm   \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "import transformers as tr\n",
    "\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "homedir = os.path.expanduser(\"~\")\n",
    "print('Root Directory for Data ',homedir)\n",
    "drop_home = '/CMCC Dropbox/Antonio Navarra'\n",
    "sys.path.insert(1, homedir + drop_home + '/ZapataLibrary/Zapata')\n",
    "\n",
    "\n",
    "#Working Directory\n",
    "wkdir =  drop_home + '/AI'\n",
    "dddir =  drop_home + '/ERA5_DATA'\n",
    "\n",
    "os.chdir(homedir + wkdir)\n",
    "print('Working Directory set to ',os.getcwd())\n",
    "\n",
    "# import klus.algorithms as al\n",
    "# # import d3s.domain as domain\n",
    "# import klus.kernels as kernels\n",
    "# # import d3s.tools as tools\n",
    "\n",
    "import zapata.computation as zcom\n",
    "import zapata.data as zd\n",
    "import zapata.lib as zlib\n",
    "import zapata.mapping as zmap\n",
    "import AIModels.AIutil as zai\n",
    "import AIModels.AIClasses as zaic\n",
    "import AIModels.ClimFormer as zcf\n",
    "import AIModels.UtilPlot as utp\n",
    "import AIModels.ModelTraining as mtr\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#%pdb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "env = zlib.get_environment_info('interpreter')\n",
    "print(f'\\n\\nNotebook operating in ENV {env}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099d384-d842-4dab-a909-6c1e848b414a",
   "metadata": {},
   "source": [
    "## Input data from ERA5\n",
    "\n",
    "Definiton of Areas and Period\n",
    "\n",
    "Choice of encoding and variance retained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961ddc5-a9d3-4c05-a202-ac225f27e795",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The training data is read using a definition of a class that defines which data are read in which field, and how many EOF, and which region of the word is selected, including level. This class is contained into the *AIutil* class module and is called **Field**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6453be-6506-4645-9d87-5ad4142de064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure case\n",
    "\n",
    "case = 'X621'\n",
    "\n",
    "shift = 'ERA5'      # Choose data period\n",
    "SMOOTH = False       # Apply seasonal filter\n",
    "period = 'ANN'      # Annual or season\n",
    "data_name= case \n",
    "normalization = 'anom'  #Normalization for the EOF\n",
    "write_result = False    # Write esults to netcdf file\n",
    "only_valtrain_eof = True  # Use only training and validation for EOF\n",
    "verify_dyn = False  #  Verify against dynamical forecasts\n",
    "choose_device = 'MPS' #'GPU'   # choose if 'CPU' or 'GPU'\n",
    "# Use SVD encoding\n",
    "encode_svd = True\n",
    "#Choose number of SVD to ratain\n",
    "var_retained = None # 0.99#.99#\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(46963291)\n",
    "\n",
    "#Select period for EOF:\n",
    "if only_valtrain_eof:\n",
    "    #Use only training and validation for EOF:\n",
    "    if SMOOTH:\n",
    "        eof_interval =['1940-03-01','2015-12-01']\n",
    "    else:\n",
    "        eof_interval =['1940-01-01','2015-12-01']\n",
    "else:\n",
    "    eof_interval = None   # Use all data\n",
    "    \n",
    "plfield = 'T2MT2M'\n",
    "\n",
    "#Select variables:\n",
    "\n",
    "# [SST,T2M], Smooth, [8,16]\n",
    "Z200 = zaic.Field('Z',['200'],'NH-ML',12)\n",
    "# V850 = zaic.Field('V',['850'],'NH-ML',10)\n",
    "# U850 = zaic.Field('U',['850'],'NH-ML',10)\n",
    "Z850 = zaic.Field('Z',['850'],'EUROPE',12)\n",
    "Z500 = zaic.Field('Z',['500'],'NH-ML',12)\n",
    "T2M = zaic.Field('t2m',['t2m'],'EUROPE',15)\n",
    "SP = zaic.Field('SP',['SP'],'WORLD',15)\n",
    "SST = zaic.Field('SST',['SST'],'WORLD',15)\n",
    "InputVars = [T2M,SST]\n",
    "PredictVars = [T2M,SST]\n",
    "\n",
    "\n",
    "\n",
    "# Write case configuration\n",
    "caseid =  {'case':case, 'version':'V5', 'SMOOTH':SMOOTH, 'normalization':normalization,\\\n",
    "           'only_valtrain_eof':only_valtrain_eof, 'eof_interval':eof_interval,\\\n",
    "           'shift':shift,'period':period, 'InputVars':InputVars,'PredictVars':PredictVars}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d6402-017f-45f9-a045-23aee0b1045a",
   "metadata": {},
   "source": [
    "The input variable are then collected within a dictionary **INX** that contains all the variables and the information that are necessary for the analysis. Another Dictionary **caseid** contains the basic information, identifying the options and the configuration of these analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8d3759-755d-4060-b10c-7d2034367298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /Users/antonionavarra/CMCC Dropbox/Antonio Navarra/ERA5_DATA/DATA_CACHE  Already Exists\n",
      "Selected data from 1940-01-01T00:00:00.000000000 to 2022-12-01T00:00:00.000000000 \n",
      "\n",
      "Use Greenwich centered coordinates with centlat=0\n",
      "Selecting field t2m for level t2m and area EUROPE\n",
      "{'dropnan': False, 'detrend': False}\n",
      "Created data Matrix X, stacked along dimensions ('lat', 'lon') \n",
      "Option DropNaN False -- Shape of Xmat (45241, 996)\n",
      "EOF interval defined -- Using data from 1940-01-01 to 2015-12-01\n",
      "make_eof: -- EOF interval defined -- Using data from 1940-01-01 to 2015-12-01\n",
      "make_eof: -- EOF interval defined -- Using data from 0 to 911\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Input variables data base\u001b[39;00m\n\u001b[1;32m      2\u001b[0m invar_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMOOTH\u001b[39m\u001b[38;5;124m'\u001b[39m:SMOOTH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalization\u001b[39m\u001b[38;5;124m'\u001b[39m:normalization, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetrend\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m,\\\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meof_interval\u001b[39m\u001b[38;5;124m'\u001b[39m:eof_interval, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshift\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERA5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase\u001b[39m\u001b[38;5;124m'\u001b[39m:case, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatatype\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource_data\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m:homedir\u001b[38;5;241m+\u001b[39mdddir}\n\u001b[0;32m----> 4\u001b[0m INX\u001b[38;5;241m=\u001b[39mzai\u001b[38;5;241m.\u001b[39mmake_data_base(InputVars, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvar_dict)\n",
      "File \u001b[0;32m~/CMCC Dropbox/Antonio Navarra/ZapataLibrary/Zapata/AIModels/AIutil.py:756\u001b[0m, in \u001b[0;36mmake_data_base\u001b[0;34m(InputVars, period, version, SMOOTH, normalization, eof_interval, detrend, shift, case, datatype, location)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEOF interval defined -- Using data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meof_interval[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meof_interval[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 756\u001b[0m     mr, varr, udat, vdat,sdat \u001b[38;5;241m=\u001b[39m make_eof(X,invar\u001b[38;5;241m.\u001b[39mmr,eof_interval\u001b[38;5;241m=\u001b[39meof_interval)\n\u001b[1;32m    757\u001b[0m dv \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase\u001b[39m\u001b[38;5;124m'\u001b[39m:case,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m:invar\u001b[38;5;241m.\u001b[39marea, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatatype\u001b[39m\u001b[38;5;124m'\u001b[39m: datatype,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m:invar\u001b[38;5;241m.\u001b[39mname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m:inlevel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentlon\u001b[39m\u001b[38;5;124m'\u001b[39m:centlon,\\\n\u001b[1;32m    758\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marealat\u001b[39m\u001b[38;5;124m'\u001b[39m:arealat, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marealon\u001b[39m\u001b[38;5;124m'\u001b[39m:arealon, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m:X,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmr\u001b[39m\u001b[38;5;124m'\u001b[39m:mr,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar_retained\u001b[39m\u001b[38;5;124m'\u001b[39m:varr,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mudat\u001b[39m\u001b[38;5;124m'\u001b[39m:udat,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvdat\u001b[39m\u001b[38;5;124m'\u001b[39m:vdat,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msdat\u001b[39m\u001b[38;5;124m'\u001b[39m:sdat}\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m (invar\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39minlevel)\u001b[38;5;241m.\u001b[39mupper()\n",
      "File \u001b[0;32m~/CMCC Dropbox/Antonio Navarra/ZapataLibrary/Zapata/AIModels/AIutil.py:284\u001b[0m, in \u001b[0;36mmake_eof\u001b[0;34m(X, mr, eof_interval)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_eof: -- EOF interval defined -- Using data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meof_interval[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meof_interval[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmake_eof: -- EOF interval defined -- Using data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meofstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meofend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m     udatx, sdatx, _ \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39msvd(xdat[:,eofstart:eofend], full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,lapack_driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgesvd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     udatx, sdatx, _ \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39msvd(xdat[:,:], full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AIDEV/lib/python3.11/site-packages/scipy/linalg/_decomp_svd.py:141\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    137\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    138\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m gesXd(a1, compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, lwork\u001b[38;5;241m=\u001b[39mlwork,\n\u001b[1;32m    142\u001b[0m                       full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices, overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Input variables data base\n",
    "invar_dict = {'period':'ANN', 'version':'V5', 'SMOOTH':SMOOTH, 'normalization':normalization, 'detrend':False,\\\n",
    "              'eof_interval':eof_interval, 'shift':'ERA5', 'case':case, 'datatype':'Source_data','location':homedir+dddir}\n",
    "INX=zai.make_data_base(InputVars, **invar_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd6406-ac74-42be-a0ab-9121973db4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Predicted variables data base\n",
    "pred_dict = {'period':'ANN', 'version':'V5', 'SMOOTH':SMOOTH, 'normalization':normalization, \\\n",
    "                   'shift':'ERA5', 'case':case, 'datatype':'Target_data','location':homedir+dddir}\n",
    "#Save space\n",
    "# PDX=zai.make_data_base(PredictVars, **pred_dict)\n",
    "PDX = INX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ce8b9-283c-41da-99d8-69a64d323c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Verification variables data base\n",
    "# SSTver = zaic.Field('SST',['SST'],'TROPIC',math.inf)\n",
    "\n",
    "# ver_dict = {'period':'ANN', 'version':'V5', 'SMOOTH':SMOOTH, 'normalization':normalization, \\\n",
    "#                    'shift':'ERA5', 'case':case, 'datatype':'Target_data','location':homedir+dddir}\n",
    "# #Save space\n",
    "# VEX=zai.make_data_base([SSTver], **ver_dict)\n",
    "plt.plot(INX['SSTSST']['vdat'][1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543d825-e063-4029-bd78-01c579d1db52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain time dates from data chosen\n",
    "field1 = list(INX.keys())[0]\n",
    "fd = INX[field1]['X'].A.time.data[0]\n",
    "fl = INX[field1]['X'].A.time.data[-1]\n",
    "pd.Timestamp(fd), pd.Timestamp(fl)\n",
    "data_time = pd.date_range(start=fd, end=fl, freq='1MS')\n",
    "data_time_str = data_time.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeac468-f080-478a-aaed-fcd9695018dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AI Model\n",
    "# Find total number of features and boundaries\n",
    "num_features_src, m_lim_src = zai.make_features(INX) \n",
    "# Find total of features in target\n",
    "num_features_tgt, m_lim_tgt = zai.make_features(PDX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67111f5-fd4b-47ff-98ff-0c64bb3bc07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params ={'TIN':48,'MIN':num_features_src,'T':12,'K':num_features_tgt,'EPOCHS':400,'RESTART':False, 'D_DIM':128, 'Tpredict':12,\n",
    "         'FFN_DIM':128,'ENC_Heads':2,'DEC_Heads':2, 'enc_dec_layers':1, 'scaling':'MaxMin',\n",
    "         'LR':0.00005,'WD':0.1,'savefile':None}\n",
    "params['savefile'] = f\"informer_{case}_{normalization}_{params['enc_dec_layers']}_{params['D_DIM']}_{params['TIN']}_{SMOOTH}\"\n",
    "\n",
    "TIN = params['TIN']  # Number of input timestep47\n",
    "MIN = params['MIN']   # Number of input features\n",
    "T = params['T']    # Number of output timesteps\n",
    "K = params['K']    # Number of predicted features\n",
    "num_epochs = params['EPOCHS'] \n",
    "RESTART = params['RESTART']\n",
    "Tpredict = params['Tpredict']\n",
    "# LAGS =[1,3,6,8,12]\n",
    "LAGS = [1,2,3,6]\n",
    "params['lags'] = LAGS\n",
    "CONTEXT = TIN - max(LAGS)\n",
    "assert CONTEXT > 0, 'Value of LAGS wrong'\n",
    "print(f' Chosen context is {CONTEXT}, for max(lags)  {max(LAGS)}')\n",
    "\n",
    "learning_rate = params['LR'] \n",
    "\n",
    "file = 'MOD_' + params['savefile'] + '.pt'\n",
    "FIGfile =  'FIG_' + params['savefile'] + '.pdf'\n",
    "\n",
    "batch_size = 32\n",
    "torch.manual_seed(17553984)\n",
    "\n",
    "match choose_device:\n",
    "    case 'CPU':\n",
    "        device = torch.device('cpu')\n",
    "        t_type = torch.float32\n",
    "    case 'MPS':\n",
    "        device = torch.device('mps')\n",
    "        t_type = torch.float32\n",
    "    case _:\n",
    "        device = torch.device('cpu')\n",
    "        t_type = torch.float32\n",
    "torch.set_default_dtype(t_type)\n",
    "params['t_type'] = t_type\n",
    "params['device'] = device\n",
    "#\n",
    "#\n",
    "# The Convention for indeces is that they point to the real date.\n",
    "# I python ranges need to be defined then it must take into account the extra 1\n",
    "data_time.get_loc(fd)\n",
    "\n",
    "start_date = data_time_str[0]\n",
    "end_training = '2005-12-01'\n",
    "\n",
    "start_val = '2006-01-01'\n",
    "end_val = '2015-12-01'\n",
    "\n",
    "test_start = '2016-01-01'\n",
    "test_end = '2022-12-01'\n",
    "\n",
    "print(f'Training \\tfrom \\t{start_date} \\tto \\t{end_training} \\nValidation \\tfrom \\t{start_val} \\tto \\t{end_val} \\nTesting \\tfrom \\t{test_start} \\tto \\t{test_end}\\n')\n",
    "\n",
    "train_period_start = data_time.get_loc(start_date)\n",
    "train_period_end = data_time.get_loc(end_training)\n",
    "val_period_start = data_time.get_loc(start_val)\n",
    "val_period_end = data_time.get_loc(end_val)\n",
    "test_period_start = data_time.get_loc(test_start)\n",
    "test_period_end = data_time.get_loc(test_end)\n",
    "\n",
    "params |= {'start_date':start_date,'end_training':end_training, 'start_val':start_val,'end_val':end_val,'test_start':test_start,'test_end':test_end, \\\n",
    "        'train_period_start':train_period_start,'train_period_end':train_period_end, 'val_period_start':val_period_start, 'val_period_end':val_period_end,'test_period_start':test_period_start, 'test_period_end':test_period_end}\n",
    "\n",
    "\n",
    "print(f'Absolute Indices for \\nTraining \\t{train_period_start}\\tto \\t{train_period_end}\\nValidation \\t{val_period_start}\\tto \\t{val_period_end}\\nTest \\t\\t{test_period_start}\\tto \\t{test_period_end}\\n')\n",
    "print(f'Range Indices for \\nTraining \\t{train_period_start}\\tto \\t{train_period_end+1}\\nValidation \\t{val_period_start}\\tto \\t{val_period_end+1}\\nTest \\t\\t{test_period_start}\\tto \\t{test_period_end+1}\\n')\n",
    "\n",
    "\n",
    "print(f'Data Extent in Training {(train_period_end-train_period_start)+1}')\n",
    "print(f'Data Extent in Validation {(val_period_end-val_period_start)+1}')\n",
    "print(f'Data Extent in Test {(test_period_end-test_period_start)+1}')\n",
    "\n",
    "\n",
    "# Define range indices for forecasting sequences\n",
    "params['train_first_fcs'] = TIN                           # First forecasted month for first forecast \n",
    "params['train_last_fcs'] = params['train_period_end'] - Tpredict   +2     # First forecasted month for last forecast \n",
    "params['val_first_fcs'] = params['val_period_start']  + TIN    \n",
    "params['val_last_fcs'] = params['val_period_end'] - Tpredict    +2    \n",
    "params['test_first_fcs'] = params['test_period_start']  + TIN\n",
    "params['test_last_fcs'] = params['test_period_end'] - Tpredict    +2   \n",
    "print(f'Starting indices for the sequences in training and forecasts\\n')\n",
    "print(f\"TESTING -- \\tFirst forecasted month for first forecast {params['test_first_fcs']} \\t--> {data_time[params['test_first_fcs']]}\\\n",
    "                    \\n\\t\\tFirst forecasted month for last forecast {params['test_last_fcs']} \\t--> {data_time[params['test_last_fcs']]}\")\n",
    "print(f\"TRAINING -- \\tFirst forecasted month for first forecast {TIN}\\t--> {data_time[TIN]}\\\n",
    "                    \\n\\t\\tFirst forecasted month for last forecast {params['train_last_fcs']}\\t--> {data_time[params['train_last_fcs']]}\")\n",
    "\n",
    "print(f'Number of forecast cases in each epoch')\n",
    "print(f\"Forecast in Training {params['train_last_fcs']-params['train_first_fcs'] }\")\n",
    "print(f\"Forecast in Validation {params['val_last_fcs']-params['val_first_fcs'] }\")\n",
    "print(f\"Forecast in Test {params['test_last_fcs']-params['test_first_fcs'] }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a5e32-597c-4ce9-9ce7-29aa5d82a59d",
   "metadata": {},
   "source": [
    "The training, validation, and test are then selected in the following. The time boundaries are absolute limits for this case. They do not take into account either the input sequence or the prediction time Tpredict.\n",
    "\n",
    "The dictionary **params** contain the time information in the series that correspond to the time location in the data base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682bfb6-36d9-42b6-9f08-9bb73699911b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795c14f-e36f-4a1f-bd83-4bd8e256a56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5deb3dc-02fe-47d1-94fc-4616616480e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slim=4.0\n",
    "cont1=[-slim,slim,0.5]\n",
    "cont2=[-slim,slim,0.5]\n",
    "cont1=[]\n",
    "cont2=[]\n",
    "cont3=[]\n",
    "label1 = ' Data'\n",
    "label2 = '1983-1-1'\n",
    "label3 = '1998-1-1'\n",
    "\n",
    "plpro = 'Pacific'\n",
    "if INX[plfield]['area'] == 'EUROPE':\n",
    "    plpro = 'Atlantic'\n",
    "    \n",
    "fig,ax,pro=zmap.init_figure(len(INX.keys()),1,plpro, constrained_layout=False, figsize=(24,12) )\n",
    "for i in enumerate(INX.keys()):\n",
    "    X,arealat,arealon,centlon = zai.select_field(INX,i[1])\n",
    "    # \"x is greater\" if x > y else \"y is greater\"\n",
    "    axm = ax if len(INX.keys()) < 2  else ax[i[0]]\n",
    "    handle=zmap.xmap(X.A.sel(time=label2).unstack(),cont1, pro, ax=axm,refline=None, c_format='{:4.2f}',data_cent_lon=centlon,\\\n",
    "                 xlimit=(arealon[0],arealon[1]), ylimit=(arealat[1],arealat[0]),\n",
    "                 title={'maintitle':i[1], 'lefttitle':label2,'righttitle':label1},cmap='coolwarm',contour=False)\n",
    "    zmap.add_colorbar(fig, handle['filled'], axm, label_size=10,edges=True)\n",
    "fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "labfile =  f'{data_name}SST{period}.pdf'\n",
    "plt.savefig(labfile, orientation='landscape',  format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04105bf5-3a77-4c1f-b7a7-1efe4c2a3011",
   "metadata": {},
   "source": [
    "## Plot EOF coefficients of input fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc42bc5-4405-4a36-8039-706447b2aced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in INX.keys():\n",
    "    print(INX[i]['field'],INX[i]['udat'].shape,INX[i]['sdat'].shape,INX[i]['vdat'].shape)\n",
    "    plt.plot(INX[i]['sdat']**2/sum(INX[i]['sdat']**2),label=INX[i]['field'])\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Retained Singular values of the data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf27cd0-3fa8-4d1c-b9fc-aed202f56ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot first EOF of input fields\n",
    "\n",
    "Nplfield = INX[plfield]['mr']\n",
    "udat,_,_ = zai.select_field_eof(INX,plfield)\n",
    "nameeof = zai.select_field_key(INX,plfield,'field')\n",
    "utest = INX[plfield]['X'].A.isel(time=slice(0,Nplfield)).copy(data=udat[:,0:Nplfield])\n",
    "arealon = zai.select_field_key(INX,plfield,'arealon')\n",
    "arealat = zai.select_field_key(INX,plfield,'arealat')\n",
    "plpro = 'Pacific'\n",
    "if INX[plfield]['area'] == 'EUROPE':\n",
    "    plpro = 'Atlantic'\n",
    "slim=4.0\n",
    "cont1=[-slim,slim,0.5]\n",
    "cont2=[-slim,slim,0.5]\n",
    "cont1=[]\n",
    "cont2=[]\n",
    "label1 = 'EOF ' + nameeof\n",
    "label2 = f'EOF 1 1983-1-1'\n",
    "label3 = f'EOF {Nplfield-2} 1998-1-1'\n",
    "    \n",
    "fig,ax,pro=zmap.init_figure(2,1,plpro, constrained_layout=False, figsize=(12,12) )\n",
    "handle=zmap.xmap(utest.isel(time=0).unstack(),cont1, pro, ax=ax[0],refline=None, c_format='{:4.2f}',data_cent_lon=INX[plfield]['centlon'],\\\n",
    "                       xlimit=(arealon[0],arealon[1]), ylimit=(arealat[1],arealat[0]),\n",
    "                 title={'maintitle':'SST', 'lefttitle':label2,'righttitle':label1},cmap='coolwarm',contour=False)\n",
    "zmap.add_colorbar(fig, handle['filled'], ax[0], label_size=8,edges=True)\n",
    "# ax[1].projection = zmap.choose_projection('Atlantic')\n",
    "han1=zmap.xmap(utest.isel(time=Nplfield-2).unstack(), cont2, plpro, ax=ax[1], refline=None,c_format='{:4.2f}',data_cent_lon=INX[plfield]['centlon'],\\\n",
    "                       xlimit=(arealon[0],arealon[1]),ylimit=(arealat[1],arealat[0]),\n",
    "               title={'maintitle':'SST', 'lefttitle':label3,'righttitle':label1},cmap='coolwarm',contour=False)\n",
    "zmap.add_colorbar(fig, han1['filled'], ax[1],label_size=8,edges=True)\n",
    "fig.subplots_adjust(wspace=0.1,hspace=0.3)\n",
    "labfile =  f'EOF{data_name}SST{period}.pdf'\n",
    "# plt.savefig(labfile, orientation='landscape',  format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c4d5a-e248-42e6-b8f3-646a0bbcc83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd63fd4-cf21-4a83-ad85-487f41fbe7c0",
   "metadata": {},
   "source": [
    "## Data Preparation and Input parameters\n",
    "\n",
    "The total number of features, and the limit in the database, for the different fields are then computed in **make_features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d585ed7-d16f-456c-8394-b2d81d08b50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure AI Model\n",
    "# Find total number of features and boundaries\n",
    "num_features_src, m_lim_src = zai.make_features(INX) \n",
    "# Find total of features in target\n",
    "num_features_tgt, m_lim_tgt = zai.make_features(PDX) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef25c90-8cbb-4dae-bec8-755e7fc846a8",
   "metadata": {},
   "source": [
    "# Configure the model\n",
    "\n",
    "## Define Time limit taking into account Tpredict and predictions\n",
    "\n",
    "The model can have two different future times. *T*  is the time on which the model is trained. The second time is Tpredict. This is the extent of the prediction that is going to be produced by the greedy inference after the model is being evaluated.\n",
    "\n",
    "The prediction starting time can go only to Tpredict distance from the end training.\n",
    "And the first forecast start at time *start_test + TIN*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbcd01-e354-4f42-895f-76bac8f4edc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = (train_period_end-train_period_start)+1\n",
    "val_size = (val_period_end-val_period_start)+1\n",
    "test_size = (test_period_end-test_period_start)+1\n",
    "\n",
    "tot_size = (test_period_end-train_period_start)+1\n",
    "print(f'Train {train_size}, Validation {val_size}, Test {test_size}')\n",
    "print(f'Check {train_size+val_size+test_size} <---> {tot_size}')\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, val_data, test_data, _ = zai.make_data(INX,params)\n",
    "tgt_train_data, tgt_val_data, tgt_test_data, _ = zai.make_data(PDX,params)\n",
    "\n",
    "print(f'\\nTensors Train {train_data.shape}, Validation {val_data.shape}, Test {test_data.shape}')\n",
    "print(f'Check Tensors {train_data.shape[0]+val_data.shape[0]+test_data.shape[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640c448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create instances of the time features\n",
    "tim_train = zai.create_time_features(data_time[:train_size+1], 0, device)\n",
    "tim_val = zai.create_time_features(data_time[train_size:train_size+val_size+1], 0, device)\n",
    "tim_test = zai.create_time_features(data_time[train_size+val_size:tot_size+1], 0, device)\n",
    "\n",
    "# Create instances of the custom dataset for train, validation, and test\n",
    "train_dataset = zcf.TimeSeriesDataset(train_data, tgt_train_data, TIN, MIN, T, K, time_features=tim_train )\n",
    "val_dataset = zcf.TimeSeriesDataset(val_data, tgt_val_data, TIN, MIN, T, K, time_features=tim_val )\n",
    "test_dataset = zcf.TimeSeriesDataset(test_data, tgt_test_data,TIN, MIN, T, K, time_features=tim_test )\n",
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset,  batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea683b-cbc9-447a-8dd2-b2b5a3121c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader configuration for prediction\n",
    "# Only for the test dataset\n",
    "# Create instances of the custom dataset for train, validation, and test\n",
    "# pred_dataset = zaic.TimeSeriesDataset(test_data, tgt_test_data, tim_test,TIN, MIN, Tpredict, K)\n",
    "\n",
    "# # Create data loaders for train, validation, and test sets\n",
    "# pred_dataloader = DataLoader(pred_dataset,batch_size=batch_size)\n",
    "len(np.arange(params['test_period_start'],params['test_period_end']+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634cb2e",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61775719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing an Informer configuration with 12 time steps for prediction\n",
    "\n",
    "configuration = tr.InformerConfig(prediction_length=T, context_length = CONTEXT, \n",
    "                distribution_output = 'student_t', loss = 'nll', \n",
    "                input_size = MIN,\n",
    "                lags_sequence = LAGS,\n",
    "                scaling = 'std',\n",
    "                num_dynamic_real_features = 0, \n",
    "                num_static_real_features = 0,\n",
    "                num_static_categorical_features = 0,\n",
    "                num_time_features  = 3,\n",
    "                cardinality = None,\n",
    "                embedding_dimension = None,\n",
    "                d_model= params['D_DIM'],\n",
    "                encoder_ffn_dim = params['FFN_DIM'],\n",
    "                decoder_ffn_dim = params['FFN_DIM'],\n",
    "                encoder_attention_heads = params['ENC_Heads'],\n",
    "                decoder_attention_heads = params['DEC_Heads'],\n",
    "                encoder_layers = params['enc_dec_layers'],\n",
    "                decoder_layers = params['enc_dec_layers'],\n",
    "                is_encoder_decoder = True,\n",
    "                activation_function = 'gelu',\n",
    "                dropout = 0.2,\n",
    "                encoder_layerdrop = 0.1,\n",
    "                decoder_layerdrop= 0.1,\n",
    "                attention_dropout= 0.1,\n",
    "                activation_dropout= 0.1,\n",
    "                num_parallel_samples= 50,\n",
    "                init_std = 0.02,\n",
    "                loss_weight = None,\n",
    "                use_cache = True,\n",
    "                attention_type= 'prob',\n",
    "                sampling_factor = 8,\n",
    "                distil = True)\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = zcf.ClimFormer(configuration)\n",
    "# \n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "configuration.context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204d0a4-9482-4823-b92e-e20a0d5b0b1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6d6ba-ba6f-46c8-9205-a04464ed7988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add to configuration case the Model configuration\n",
    "caseid['model_config'] = configuration\n",
    "caseid['model_file'] = file\n",
    "caseid['params'] = params\n",
    "\n",
    "\n",
    "fileconf = f\"CONF_{params['savefile']}.npz\"\n",
    "np.savez(fileconf,caseid=caseid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d36fa-7c70-4de0-b5c3-01145366a3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for a,b,c,d  in iter(val_dataloader):\n",
    "    print(a.shape,b.shape,c.shape,d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3b8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# criterion = nn.L1Loss()\n",
    "\n",
    "#Optimizer\n",
    "optim1 = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=params['WD'],amsgrad=True)\n",
    "optim2 = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=params['WD'],amsgrad=True)\n",
    "optim4 = optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=5*params['WD'])\n",
    "\n",
    "\n",
    "#Number of parameters\n",
    "print(f'The model has {zai.count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe0e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train and validate model\n",
    "clip=1\n",
    "check_val = 1\n",
    "patience=20\n",
    "\n",
    "if RESTART:\n",
    "    model.load_state_dict(torch.load(file))\n",
    "    \n",
    "best_val_loss = float(\"inf\")\n",
    "num_epochs = params['EPOCHS']\n",
    "loss_tr = []\n",
    "loss_va = []\n",
    "\n",
    "optims = [ optim1, optim2, optim4]\n",
    "rates = [learning_rate, learning_rate, learning_rate]\n",
    "\n",
    "for i in range(len(optims)):    \n",
    "\n",
    "    print(optims[i])\n",
    "    early_stopping = zaic.EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optims[i], patience=patience, factor=0.5, mode=\"min\", verbose=True)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Train the model\n",
    "        trained_model,losstr = mtr.train_model(model, epoch,  train_dataloader, optims[i],\n",
    "                                lr=rates[i], patience=patience, clip=1.0,device=device)\n",
    "        loss_tr.append(losstr)\n",
    "        if epoch % check_val == 0:\n",
    "            trained_model,lossva = mtr.validate_model(model, epoch,  val_dataloader, \n",
    "                                lr=rates[i], patience=patience, clip=1.0,device=device)\n",
    "            scheduler.step(lossva)\n",
    "            loss_va.append(lossva)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            scheduler.step(lossva)\n",
    "            # Early stopping\n",
    "            if early_stopping(lossva):\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "            if lossva < best_val_loss:\n",
    "                best_val_loss = lossva\n",
    "                print(f'Saving file to {file} at loss {lossva:.5f}')\n",
    "                torch.save(model.state_dict(), file)\n",
    "\n",
    "            print(f'\\t Epoch {epoch} --   Train Loss: {losstr:.5f} --- Valid Loss: {lossva:.5f}')\n",
    "\n",
    "            \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56327eec-0570-4bfb-a0bb-3ce4b58fc38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tpredict = 12\n",
    "# params['Tpredict'] = Tpredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbeaf7d-ad3d-47f6-8ecf-079e1465d5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create data loaders for prediction with Tpredict length for the future features\n",
    "# Create instances of the custom dataset for train, validation, and test\n",
    "train_dataset_fut = zcf.TimeSeriesFuture(train_data, tgt_train_data, TIN, MIN, T, K, Tpredict, time_features=tim_train )\n",
    "val_dataset_fut = zcf.TimeSeriesFuture(val_data, tgt_val_data, TIN, MIN, T, K, Tpredict, time_features=tim_val )\n",
    "test_dataset_fut = zcf.TimeSeriesFuture(test_data, tgt_test_data,TIN, MIN, T, K, Tpredict, time_features=tim_test )\n",
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "train_dataloader_fut = DataLoader(train_dataset_fut,  batch_size=batch_size)\n",
    "val_dataloader_fut = DataLoader(val_dataset_fut, batch_size=batch_size)\n",
    "test_dataloader_fut = DataLoader(test_dataset_fut,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2f537-ad4c-41b4-a561-928a9608b81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creates Plots\n",
    "\n",
    "# model.load_state_dict(torch.load(file))\n",
    "\n",
    "out_test = mtr.predict( model, test_dataloader_fut,Tpredict, device=device).cpu().mean(axis=1)\n",
    "out_val = mtr.predict( model, val_dataloader_fut,Tpredict,device=device).cpu().mean(axis=1)\n",
    "out_train = mtr.predict( model, train_dataloader_fut,Tpredict,device=device).cpu().mean(axis=1)\n",
    "\n",
    "out_test = out_test.cpu().detach().numpy()\n",
    "out_val = out_val.cpu().detach().numpy()\n",
    "out_train = out_train.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a40182-5b9e-48f8-86f9-3f7efc8c0a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a,b,c,d = next(iter(test_dataloader_fut))\n",
    "# output = model.generate(\n",
    "#                 past_values=a,\n",
    "#                 past_time_features=c,\n",
    "#                 past_observed_mask=None,\n",
    "#                 future_time_features=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc388d0-d460-48cf-936b-001cd5af1d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c713b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indt2m= PDX[plfield]['index']\n",
    "out_train_tot, out_val_tot, out_test_tot, true_tot = \\\n",
    "           zai.rescale(params,PDX, out_train, out_val, out_test)\n",
    "            # zai.rescale(params,PDX, out_train[:,:,indt2m], out_val[:,:,indt2m], out_test[:,:,indt2m])\n",
    "out_test_p = out_test_tot[...,indt2m] \n",
    "out_train_p = out_train_tot[...,indt2m] \n",
    "out_val_p = out_val_tot[...,indt2m] \n",
    "truep = true_tot[..., indt2m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d0935-5bbf-430d-bc88-8e00de770d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(f'LOSS_{case}_{TIN}.npy',caseid=caseid, loss_va=np.asarray(loss_va),loss_tr=np.asarray(loss_tr))\n",
    "print(f'Saving LOSS_{case}_{TIN}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b8c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_va,color='coral',label='Validation Loss')\n",
    "plt.plot(loss_tr,color='lightblue',label='Training Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "plt.show()\n",
    "\n",
    "times = data_time\n",
    "# Plot the first subplot for the month monpred\n",
    "monpred = 1\n",
    "\n",
    "starttr = params['train_first_fcs']\n",
    "endtr = params['train_last_fcs']\n",
    "startval = params['val_first_fcs']\n",
    "endval = params['val_last_fcs']\n",
    "starttest = params['test_first_fcs']\n",
    "endtest = params['test_last_fcs']\n",
    "print(starttr,endtr,startval,endval,starttest,endtest)\n",
    "print(f'Initial sequence of {TIN} months, the Training   starts at: \\t{times[starttr]}')\n",
    "print(f'                                      Validation starts at: \\t{times[startval]}')\n",
    "print(f'                            First Forecast at:\\t{times[starttest]} valid until \\t{times[starttest+Tpredict]}')\n",
    "print(f'                            Initial Sequence from:\\t{times[starttest-TIN]} ({starttest-TIN}) to\\t{times[starttest-1]} ({starttest-1})')\n",
    "print(f'                            First IC Forecast at:\\t{times[starttest-1]} valid until \\t{times[starttest-1+Tpredict]}')\n",
    "\n",
    "\n",
    "\n",
    "tit ='SST'\n",
    "def plot_result(ax1,out_train,out_val,out_test,monpred,neof,leof,tit,true=None):\n",
    "    \n",
    "    ax1.plot(times,true[:,neof], color='red', label=leof +' - True')\n",
    "    ax1.plot(times[starttr:endtr],out_train[:,monpred,neof], color='black', label=leof +' - Train')\n",
    "    ax1.plot(times[startval:endval],out_val[:,monpred,neof], color='green', label=leof +' - Val')\n",
    "    ax1.plot(times[starttest:endtest],out_test[:,monpred,neof], color='blue', label=leof +' - Test')\n",
    "    # ax1.set_xlim(times[1500],times[1600])\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel(leof)\n",
    "    ax1.set_title(tit)\n",
    "    ax1.axvline(times[train_size],color='green',lw=2.0)\n",
    "    ax1.axvline(times[train_size+val_size],color='green',lw=2.0,linestyle='dashed')\n",
    "    _, yt = ax1.get_ylim()\n",
    "    ax1.text(times[150],yt+1,'TRAINING')\n",
    "    ax1.text(times[train_size],yt+1,'VALIDATION')\n",
    "    ax1.text(times[train_size+val_size],yt+1,'FORECAST')\n",
    "    # ax1.set_xlim((times[1500],times[1600]))\n",
    "    ax1.legend(loc='upper left',fontsize='x-small')\n",
    "# Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "# plot_result(ax1,out_train_S,out_val_S,out_test_S,monpred,0,'EOF 1',tit)\n",
    "# plot_result(ax2,out_train_S,out_val_S,out_test_S,monpred,1,'EOF 2',tit)\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels and titles\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "# plt.show()\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "titz = tit\n",
    "plot_result(ax1,out_train_p,out_val_p,out_test_p,monpred,0,'ZEOF 1',titz,true=truep)\n",
    "plot_result(ax2,out_train_p,out_val_p,out_test_p,monpred,1,'ZEOF 2',titz,true=truep)\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels and titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5f41a-93e8-4e96-aec6-1ac9a63d479b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "plot_result(ax1,out_train_p,out_val_p,out_test_p,monpred,2,'EOF 3',titz,true=truep)\n",
    "plot_result(ax2,out_train_p,out_val_p,out_test_p,monpred,3,'EOF 4',titz,true=truep)\n",
    "# Adjust the layout to avoid overlapping labels and titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab06b32-402b-4128-9e29-4b64747990e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))\n",
    "plot_result(ax1,out_train_p,out_val_p,out_test_p,monpred,10,'EOF 11',titz,true=truep)\n",
    "plot_result(ax2,out_train_p,out_val_p,out_test_p,monpred,6,'EOF 7',titz,true=truep)\n",
    "# Adjust the layout to avoid overlapping labels and titles\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57a440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create arrays of ordered forecasts and verification fields\n",
    "\n",
    "# F, V, P, Obs = zai.eof_to_grid('test','SSTSST', truep, out_test_p, times, INX=PDX, params=params, truncation=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b90a1-261a-4f77-a876-7b653d54f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cont = namedtuple('Contours', 'max min int')\n",
    "cont_ = Cont(4, -3, 0.5)\n",
    "cont_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2090659-3e2b-4566-bf66-9431d606665f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cont = [-3, 3, 0.25]\n",
    "\n",
    "# plfield='SSTSST'\n",
    "arealatz = zai.select_field_key(INX,plfield,'arealat')\n",
    "arealonz = zai.select_field_key(INX,plfield,'arealon')\n",
    "# utp.many_plots(V,'T2M', 'T2M', cont, '2021-01-01', mode='lead',title='', mainlabel='Verification ',lead=0,nrows=3,ncols=2, arealatz=arealatz, arealonz=arealonz,  centlon=PDX[plfield]['centlon'],labfile='Test1.pdf')\n",
    "# utp.many_plots(F,'T2M', 'T2M', cont, '2021-01-01', mode='time',title='',mainlabel='Forecasts ', lead=1,nrows=3,ncols=2, arealatz=arealatz, arealonz=arealonz, centlon=PDX[plfield]['centlon'],labfile='Test2.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b849ee-19e6-4749-a406-bb622ad2313d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "F, V, P, Obs = zai.eof_to_grid_new('test','T2MT2M', truep, out_test_p, times, INX=PDX, params=params, truncation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767f716-8289-4ded-9e4d-49d80db23d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515a8ff-be0b-4216-aba9-0758061f61d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verify_dyn = True # check if the starting date is within the limit of the dynamicalforecasts\n",
    "# Always use the best overlap period\n",
    "dyn_startdate = pd.Timestamp('2018-11-01')\n",
    "dyn_enddate = times[endtest+Tpredict-2]\n",
    "\n",
    "if times[starttest] > dyn_startdate:\n",
    "    dyn_startdate = times[starttest]\n",
    "    \n",
    "print(f'Dynamical forecasts from {dyn_startdate} to {dyn_enddate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f1e63-588d-4f04-80ac-145f053f70ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read dynamical forecast and compute correlations\n",
    "# Forecast for different lead times have dates that correspond to the verification time.\n",
    "endindex=np.where(times == dyn_enddate)\n",
    "startindex = np.where(times == dyn_startdate)\n",
    "\n",
    "dyn_cmcc = {'center':'cmcc','system':['3','35'], 'SST':'SST', 'T2M':'2t','startdate':dyn_startdate.strftime('%Y-%m-%d'),'enddate':'2022-05-01'}\n",
    "\n",
    "dyn_cases = [dyn_cmcc]\n",
    "\n",
    "ver_f = INX[plfield]['field'].upper()\n",
    "ndyn = int(endindex[0]-startindex[0]+1)\n",
    "ngrid = INX[plfield]['X'].A.shape[0]\n",
    "dynddr =  homedir + drop_home + '/ERA5/SEASONAL_'+ ver_f\n",
    "\n",
    "filever = 'DYN_' + ver_f + params['savefile']   + '.nc'\n",
    "\n",
    "if verify_dyn:\n",
    "    DYN = zai.make_dyn_verification(ver_f,INX[plfield]['area'], dyn_cases, dynddr, times, dyn_startdate,dyn_enddate, filever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4a5d6-45f0-408a-b67d-d70041015bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area = INX[plfield]['area']\n",
    "# for lead in range(0,6):\n",
    "#     cen=dyn_cmcc\n",
    "#     vfiles = [f\"{dynddr}/{cen[ver_f]}_{cen['center']}_{k}_{lead+1}.nc\" for k in cen['system']]\n",
    "#     print(vfiles)\n",
    "#     ddd = xr.open_mfdataset(vfiles).rename({'longitude':'lon','latitude':'lat'}).sel(time=slice(times[startindex[0]+lead][0],times[endindex[0]+lead][0])).mean(dim='number')\n",
    "#     sel_field = zai.select_area(area,ddd).stack(z=('lat','lon')).transpose('time','z')\n",
    "# DYN = sel_field.expand_dims(dim={\"lead\":range(1,7)}, axis=1)['ssta']               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4684c-7369-4726-b251-efe94bac4d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb71b2-f0d1-4d63-9249-b4fb8846866c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coompute arrays for dynamical model verification\n",
    "if verify_dyn:\n",
    "    SD = DYN\n",
    "    # Stack and restack to get the NaN as the dynamics files\n",
    "    VD = V.sel(time=slice(dyn_startdate,'2022-05-01')).unstack().stack(z=(\"lat\", \"lon\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b8829-1127-4fd5-9107-718dfc480764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if write_result:\n",
    "    # Write forecast and verification\n",
    "    case_data =  xr.Dataset({'F': F,'V': V,'P':P, 'SD':SD,'VD':VD})\n",
    "    filedyn = f\"RES_{params['savefile']}.nc\"\n",
    "\n",
    "    import cf_xarray as cfxr\n",
    "    xr.set_options(keep_attrs=True)\n",
    "    cfxr.encode_multi_index_as_compress(case_data, 'z').to_netcdf(filedyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48269257-63be-4319-8e41-1236673db04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust for the IC to be consistent with the convention. \n",
    "# Consider the the GCM has onyl the first month.\n",
    "\n",
    "first = np.datetime64(DYN.time[0].data[()],'M')\n",
    "last = np.datetime64(DYN.time[-1].data[()],'M')\n",
    "\n",
    "date_array = np.arange(first-np.timedelta64(1, 'M'), last , np.timedelta64(1, 'M'))\n",
    "\n",
    "# Convert to 'D' precision to include only the first day of each month\n",
    "date_array_days = date_array.astype('datetime64[D]')\n",
    "DYN = DYN.assign_coords(time=date_array_days)\n",
    "\n",
    "# Add Initial conditions to the dynamical forecasts\n",
    "IC = V[:,0,:].expand_dims('lead',axis=1)\n",
    "NEW_DYN = xr.concat([IC,DYN],dim='lead')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc6fce-b961-413d-a622-ea8d3d044c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "Cont = namedtuple('Contours', 'max min int')\n",
    "cont_ = Cont(4, -4, 0.25)\n",
    "np.arange(cont_.min,cont_.max,cont_.int)\n",
    "cont=[cont_.min, cont_.max, cont_.int]\n",
    "\n",
    "# cont = []\n",
    "if Tpredict > 5:\n",
    "    utp.Single_Forecast_plots(F, V,  'SST', None, cont, '2021-07-01', maxtime=6, stride=2, title='ClimFormer Verification', colorbars=False, mainlabel='', arealatz=arealatz, arealonz=arealonz, centlon=PDX[plfield]['centlon'],labfile='Fcst.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60916acf-0dde-4f27-b5f4-a1f49874de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "utp.Single_Forecast_plots(NEW_DYN, V,  'SST', None, cont, '2021-07-01', maxtime=6, stride=2, title='GCM Verification', colorbars=False, mainlabel='', arealatz=arealatz, arealonz=arealonz, centlon=PDX[plfield]['centlon'],labfile='FcstTrunc.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe53162-337b-4978-80c0-93e754daa724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beed310-3f5e-4ad7-a4cb-e45becac963a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute correlation for dynamical model\n",
    "# For each day\n",
    "    \n",
    "if verify_dyn:\n",
    "    ERROR = 'L2'\n",
    "    Tpredict_dyn = 7\n",
    "\n",
    "    corrdyn = np.zeros((Tpredict_dyn-1,SD.shape[0]))\n",
    "    rmsdyn = np.zeros((Tpredict_dyn-1,SD.shape[0]))\n",
    "\n",
    "    totday,totfor = corrdyn.shape\n",
    "    print(SD.shape,VD.shape,corrdyn.shape)\n",
    "    # The number of forecast for the dynamical list is limited to 7 months, so the number of cases must be reduced.\n",
    "    number_of_dyn = VD.shape[0]\n",
    "\n",
    "    # for icj in range(totfor): \n",
    "    #     print(f'Treating IC ---> {icj}')\n",
    "    #     corrdyn[:,icj]= xr.corr(SD[icj,:,:],VD[icj,:-1,:], dim='z').data   \n",
    "    #     NumPoints = F.shape[2]\n",
    "    #     if ERROR == 'L1':\n",
    "    #         rmsdyn[:,icj]= np.sum(abs(SD[icj,:,:] - VD[icj,:-1,:]),axis=1).data/NumPoints\n",
    "    #     else: \n",
    "    #         rmsdyn[:,icj]= np.sqrt(np.sum((SD[icj,:,:] - VD[icj,:-1,:])**2,axis=1).data)/NumPoints\n",
    "\n",
    "    for icj in range(totday): \n",
    "            print(f'Treating Day ---> {icj}')\n",
    "            corrdyn[icj,:number_of_dyn]= xr.corr(SD[:number_of_dyn,icj,:],VD[:,icj,:], dim='z').data   \n",
    "            NumPoints = F.shape[2]\n",
    "            if ERROR == 'L1':\n",
    "                rmsdyn[icj,:number_of_dyn]= np.sum(abs(SD[:number_of_dyn,icj,:] - VD[:,icj,:]),axis=1).data/NumPoints\n",
    "            else:\n",
    "                rmsdyn[icj,:number_of_dyn]= np.sqrt(np.sum((SD[:number_of_dyn,icj,:] - VD[:,icj,:])**2,axis=1).data)/NumPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c686e77-9cbb-411a-a44f-fdf2419b5bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded2315-1d72-4ee9-91b4-6dbed9665e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titolo = namedtuple('Titolo', 'maintitle lefttitle righttitle')\n",
    "a = Titolo('SST','Exp1','EOF1')\n",
    "a.maintitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de722be-5f31-48d5-b110-c4a9fa2ed9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c370ff-5cb1-4be5-89d5-63eef9a25955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation\n",
    "# For each day\n",
    "\n",
    "ERROR = 'L2'\n",
    "Np, _, _ = F.shape\n",
    "\n",
    "corrresult = np.zeros((Tpredict+1,Np))\n",
    "persistence = np.zeros((Tpredict+1,Np))\n",
    "\n",
    "rmsres = np.zeros((Tpredict+1,Np))\n",
    "rmsper = np.zeros((Tpredict+1,Np))\n",
    "\n",
    "# totday,totfor = corrresult.shape\n",
    "\n",
    "ictime = Obs.time\n",
    "starttest = params['test_period_start'] \n",
    "\n",
    "for iday in range(Tpredict+1):\n",
    "    this_day = ictime[starttest+1+iday:starttest+iday+Np+1]\n",
    "    Fp = F[:,iday,:].assign_coords({'time':this_day.data})\n",
    "    Vp = V[:,iday,:].assign_coords({'time':this_day.data})\n",
    "    Pp = P[:,iday,:].assign_coords({'time':this_day.data})\n",
    "    corrresult[iday,:]= xr.corr(Fp,Vp, dim='z').data\n",
    "    persistence[iday,:]= xr.corr(Pp,Vp, dim='z').data\n",
    "    if verify_dyn:\n",
    "        dyn = np.mean(corrdyn,axis=1)  # Start from month 1\n",
    "    \n",
    "    NumPoints = F.shape[2]\n",
    "    if ERROR == 'L1':\n",
    "        rmsresi[iday,:]= np.sum(abs(Fp - Vp),axis=2).data/NumPoints\n",
    "        rmsper[iday,:]= np.sum(abs(Pp - Vp),axis=2).data/NumPoints\n",
    "    else:\n",
    "        rmsres[iday,:]= np.sqrt(np.sum((Fp - Vp)**2,axis=1).data)/NumPoints\n",
    "        rmsper[iday,:]= np.sqrt(np.sum((Pp - Vp)**2,axis=1).data)/NumPoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c31dfd-598d-4602-9ae7-ec0918a9baf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# plfield = 'SSTSST'\n",
    "starttest = params['test_period_start']  + TIN\n",
    "endtest = starttest + Np - 1\n",
    "\n",
    "figarea = INX[plfield]['area']\n",
    "figinfield = [i for i in INX.keys()]\n",
    "idfig = f\"FIELDS={figinfield}, MIN={MIN},MOUT={K},in={TIN},FD={params['FFN_DIM']},H={params['ENC_Heads']},LY={params['enc_dec_layers']}\"\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "for i in range(1,Tpredict,5):\n",
    "    hl, = ax1.plot(times[starttest:endtest+1],corrresult[i,:],label=f'Month {i}')\n",
    "    ax1.plot(times[starttest:endtest+1],persistence[i,:],label=f'Pers {i}',linestyle='dashed',color=hl.get_color())\n",
    "ax1.set_ylim(-0.4,1.2)\n",
    "ax1.set_title(f'Skill Score {plfield} in {figarea}',loc='left')\n",
    "ax1.set_title(idfig,loc='right',fontsize=8)\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "    \n",
    "tw = [sum(np.where(corrresult[i,:] > 0.6, 1,0))/Np for i in range(Tpredict)]\n",
    "ax3.bar(np.arange(1,Tpredict+1),tw)\n",
    "ax3.set_title('Number of forecasts above 0.6')\n",
    "ax3.set_xlabel('Lead Months')\n",
    "# plt.savefig('Bar'+idfig+'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd673d-1eba-42b1-b4aa-022029d5ff9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot only from Forecast Month 1 because of problems with identifying exactly the IC in the case of dynamic forecast.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "\n",
    "sk = np.mean(corrresult,axis=1)\n",
    "skstd = np.std(corrresult,axis=1)\n",
    "ps = np.mean(persistence,axis=1)\n",
    "\n",
    "if verify_dyn:\n",
    "    dyn = np.mean(corrdyn,axis=1)  # Start from month 1\n",
    "    dystd = np.std(corrdyn,axis=1)\n",
    "\n",
    "tim = np.arange(0,Tpredict+1)\n",
    "# print(tim.shape,sk.shape)\n",
    "ax2.plot(tim, sk,label='FCST',color='blue')\n",
    "ax2.plot(tim, ps,linestyle='dashed',color='coral',label='PERS')\n",
    "if verify_dyn:\n",
    "    ax2.plot(tim[1:7], dyn,linestyle='dashed',color='lightgreen',label='DYN')\n",
    "\n",
    "ax2.plot(tim,sk+skstd,color='blue',alpha=0.5,linestyle='dashed')\n",
    "ax2.plot(tim,sk-skstd,color='blue',alpha=0.5,linestyle='dashed')\n",
    "# ax2.plot(tim[:-1],(dyn+dystd)[1:],color='lightgreen',alpha=0.5)\n",
    "# ax2.plot(tim[:-1],(dyn-dystd)[1:],color='lightgreen',alpha=0.5)\n",
    "\n",
    "\n",
    "ax2.axhline(0.6,linestyle='dashed')\n",
    "ax2.set_title(f'ACC vs ERA5 {figarea}',loc='left')\n",
    "ax2.set_title(idfig,loc='right',fontsize=8)\n",
    "for ii in range(0,Tpredict+1):\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.1,f'{sk[ii]:4.2f}',horizontalalignment='center',color='blue')\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.2,f'{ps[ii]:4.2f}',horizontalalignment='center',color='coral')\n",
    "ax2.legend(loc='upper right')\n",
    "# ax2.set_xlim(1,7)    \n",
    "\n",
    "sk = np.mean(rmsres,axis=1)\n",
    "skstd = np.std(rmsres,axis=1)\n",
    "ps = np.mean(rmsper,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "if verify_dyn:\n",
    "    rdyn = np.mean(rmsdyn,axis=1)\n",
    "    ax1.plot(tim[1:7], rdyn,label='DYN',color='lightgreen')\n",
    "ax1.plot(tim, sk,label='FCST',color='blue')\n",
    "ax1.plot(tim,ps,linestyle='dashed',color='coral',label='PERS')\n",
    "\n",
    "ax1.plot(tim,sk+skstd,color='pink',alpha=0.5)\n",
    "ax1.plot(tim,sk-skstd,color='pink',alpha=0.5)\n",
    "\n",
    "ax1.set_title(f'RMS vs ERA5 {figarea}',loc='left')\n",
    "ax1.set_title(idfig,loc='right',fontsize=8)\n",
    "\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# plt.savefig('Skill'+idfig +'.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8af0e-55e3-438f-9a82-86197304a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724f9b7-e2c5-49fc-9666-a609bd98f802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skill = 'median' #'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cb5be-4bb7-471e-b656-d52f8ee8e5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2,ax3) = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "for i in range(1,Tpredict,5):\n",
    "    hl,=ax1.plot(times[starttest:endtest+1],corrresult[i,:],label=f'Month {i}')\n",
    "    ax1.plot(times[starttest:endtest+1],persistence[i,:],label=f'Month {i}',color=hl.get_color(), linestyle='dashed')\n",
    "idfig = f\"FIELDS={figinfield}, MIN={MIN},MOUT={K},in={TIN},FD={params['FFN_DIM']},H={params['ENC_Heads']},LY={params['enc_dec_layers']}\"\n",
    "    \n",
    "ax1.set_ylim(-0.4,1.2)\n",
    "ax1.set_title('Skill Score Median',loc='left')\n",
    "ax1.set_title(idfig,loc='right',fontsize=10)\n",
    "ax1.legend()\n",
    "\n",
    "tim = np.arange(0,Tpredict+1)\n",
    "\n",
    "match skill:\n",
    "    case 'mean':\n",
    "        sk = np.mean(corrresult,axis=1)\n",
    "        ps = np.mean(persistence,axis=1)\n",
    "        if verify_dyn:\n",
    "            dyn = np.mean(corrdyn,axis=1)  # Start from month 1\n",
    "        ax2.plot(tim,corrresult.mean(axis=1)+corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "        ax2.plot(tim,corrresult.mean(axis=1)-corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "    case 'median':\n",
    "        sk = np.median(corrresult,axis=1)\n",
    "        ps = np.median(persistence,axis=1)\n",
    "        if verify_dyn:\n",
    "            dyn = np.median(corrdyn,axis=1)  # Start from month 1\n",
    "        ax2.plot(tim,np.median(corrresult,axis=1)+corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "        ax2.plot(tim,np.median(corrresult,axis=1)-corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "    case _:\n",
    "        raise ValueError('Invalid skill choice')\n",
    "ax2.plot(tim,sk)\n",
    "ax2.plot(tim,ps,linestyle='dashed')\n",
    "if verify_dyn:\n",
    "    ax2.plot(tim[1:7],dyn,linestyle='dashed',color='green')\n",
    "        \n",
    "ax2.axhline(0.6,linestyle='dashed')\n",
    "ax2.set_title('Skill score Median',loc='left')\n",
    "ax2.set_title('Verification total field',loc='right')\n",
    "for ii in range(0,Tpredict+1):\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.1,f'{sk[ii]:4.2f}',horizontalalignment='center')\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.2,f'{ps[ii]:4.2f}',horizontalalignment='center',color='coral')\n",
    "    \n",
    "tw = [sum(np.where(corrresult[i,:] > 0.6, 1,0))/Np for i in range(Tpredict)]\n",
    "ax3.bar(np.arange(1,Tpredict+1),tw)\n",
    "ax3.set_title('Number of forecasts above 0.6')\n",
    "ax3.set_xlabel('Months')\n",
    "plt.savefig(f'FIG_{file}_{skill}.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145b1d7-f502-4c1c-b664-4fe6a8f8e8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f3610c-c586-455b-b4c8-90685a70509e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figure for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a13b69-3d2a-46ff-b937-b0897ee61c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "idfig = f\"F={figinfield}, LAGS={params['lags']}, MIN={MIN},in_seq={TIN},FD={params['FFN_DIM']},H={params['ENC_Heads']},LY={params['enc_dec_layers']}\"\n",
    "\n",
    "# Create a box plot with customization\n",
    "plt.figure(figsize=(8, 6))\n",
    "# box = plt.boxplot(corrresult.T, vert=True, patch_artist=True, notch=False, widths=0.2)\n",
    "# box1 = plt.boxplot(persistence.T, vert=True, patch_artist=True, notch=False, widths=0.2)\n",
    "\n",
    "#Customizing box plot appearance\n",
    "\n",
    "# Create a box plot with different colors for each dataset\n",
    "box1 = plt.boxplot(corrresult[1:,:].T, patch_artist=True,\n",
    "                  boxprops=dict(facecolor='lightblue', color='blue'),\n",
    "                  medianprops=dict(color='darkblue',linewidth=2),\n",
    "                  whiskerprops=dict(color='blue'),\n",
    "                  capprops=dict(color='blue'),\n",
    "                  flierprops=dict(markeredgecolor='blue'),\n",
    "                  )\n",
    "box2 = plt.boxplot(persistence[1:,:].T, patch_artist=True,\n",
    "                  boxprops=dict(facecolor='lightgreen', color='green'),\n",
    "                  medianprops=dict(color='darkgreen',linewidth=2),\n",
    "                  whiskerprops=dict(color='green'),\n",
    "                  capprops=dict(color='green'),\n",
    "                  flierprops=dict(markeredgecolor='green'),\n",
    "                  )\n",
    "\n",
    "\n",
    "if verify_dyn:\n",
    "    box3 = plt.boxplot(corrdyn.T, patch_artist=True,\n",
    "                       widths=0.25,\n",
    "                  boxprops=dict(facecolor='lightpink', color='red'),\n",
    "                  medianprops=dict(color='darkred',linewidth=2),\n",
    "                  whiskerprops=dict(color='red'),\n",
    "                  capprops=dict(color='red'),\n",
    "                  flierprops=dict(markeredgecolor='red'),\n",
    "                  )\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0],box3[\"boxes\"][0]], ['Model', 'Persistence','GCM'], loc='upper right')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('ACC Skill Score',loc='left')\n",
    "plt.title(idfig,loc='right',fontsize=8)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Spatial ACC')\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(f'FIG_{file}_Box.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2033a9d-36ff-4c42-b6c2-08db8c97176c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de1141-4457-457c-bebc-0ddba41733b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, (ax2, ax3) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# for i in range(1,Tpredict,5):\n",
    "#     hl,=ax1.plot(times[starttest:endtest+1],corrresult[i,:],label=f'Month {i}')\n",
    "#     ax1.plot(times[starttest:endtest+1],persistence[i,:],label=f'Month {i}',color=hl.get_color(), linestyle='dashed')\n",
    "# idfig = f\"FIELDS={figinfield}, MIN={MIN},MOUT={K},in={TIN},FD={params['FFN_DIM']}\"\n",
    "    \n",
    "# ax1.set_ylim(-0.4,1.2)\n",
    "# ax1.set_title('Skill Score Median',loc='left')\n",
    "# ax1.set_title(idfig,loc='right',fontsize=10)\n",
    "# ax1.legend()\n",
    "skill='median'\n",
    "\n",
    "tim = np.arange(0,Tpredict+1)\n",
    "\n",
    "match skill:\n",
    "    case 'mean':\n",
    "        sk = np.mean(corrresult,axis=1)\n",
    "        ps = np.mean(persistence,axis=1)\n",
    "        if verify_dyn:\n",
    "            dyn = np.mean(corrdyn,axis=1)  # Start from month 1\n",
    "        ax2.plot(tim,corrresult.mean(axis=1)+corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "        ax2.plot(tim,corrresult.mean(axis=1)-corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "    case 'median':\n",
    "        sk = np.median(corrresult,axis=1)\n",
    "        ps = np.median(persistence,axis=1)\n",
    "        if verify_dyn:\n",
    "            dyn = np.median(corrdyn,axis=1)  # Start from month 1\n",
    "        ax2.plot(tim,np.median(corrresult,axis=1)+corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "        ax2.plot(tim,np.median(corrresult,axis=1)-corrresult.std(axis=1),color='pink',alpha=0.5)\n",
    "    case _:\n",
    "        raise ValueError('Invalid skill choice')\n",
    "ax2.plot(tim,sk)\n",
    "ax2.plot(tim,ps,linestyle='dashed')\n",
    "if verify_dyn:\n",
    "    ax2.plot(tim[1:7],dyn,linestyle='dashed',color='green')\n",
    "        \n",
    "ax2.axhline(0.6,linestyle='dashed')\n",
    "ax2.set_title('Skill score Median',loc='left')\n",
    "ax2.set_title(idfig,loc='right',fontsize=8)\n",
    "for ii in range(0,Tpredict+1):\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.1,f'{sk[ii]:4.2f}',horizontalalignment='center',color='blue')\n",
    "    ax2.text(ii,min(ax2.get_ylim())+0.2,f'{ps[ii]:4.2f}',horizontalalignment='center',color='coral')\n",
    "    \n",
    "tw = [sum(np.where(corrresult[i,:] > 0.6, 1,0))/Np for i in range(Tpredict)]\n",
    "ax3.bar(np.arange(1,Tpredict+1),tw)\n",
    "ax3.set_title('Number of forecasts above 0.6')\n",
    "ax3.set_xlabel('Months')\n",
    "plt.savefig(f'FIG_{file}_{skill}.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b48901-305e-4e34-a53a-f099e1966ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d5524-f48b-4eb5-892d-895f790b7bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Figure for forecast\n",
    "\n",
    "# Create a figure with GridSpec layout\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = fig.add_gridspec(3, 2, width_ratios=[4, 1], height_ratios=[1, 1, 1], wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Left panels\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "# Right panel, aligned with the middle left panel\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "tit = f'{plfield[0:2]}'\n",
    "if_time = [1,3]\n",
    "col = ['black','red','lightblue', 'blue','green']\n",
    "\n",
    "for i in if_time:\n",
    "    hl,=ax1.plot(times[starttest+i:endtest+1+i],corrresult[i,:],label=f'Month {i}')\n",
    "    ax1.plot(times[starttest+i:endtest+1+i],persistence[i,:],color=hl.get_color(), linestyle='dashed')\n",
    "\n",
    "ax1.set_xlim(times[starttest], times[endtest+Tpredict-1])\n",
    "ax1.set_ylim(-0.6,1.0)\n",
    "ax1.set_title(tit,loc='left')\n",
    "ax1.set_title(idfig,loc='right',fontsize=8)\n",
    "# ax1.set_title(idfig,loc='right',fontsize=10)\n",
    "ax1.legend()\n",
    "\n",
    "if_time = [6,9]\n",
    "for i in if_time:\n",
    "    hl,=ax2.plot(times[starttest+i:endtest+1+i],corrresult[i,:],label=f'Month {i}')\n",
    "    ax2.plot(times[starttest+i:endtest+1+i],persistence[i,:],color=hl.get_color(), linestyle='dashed')\n",
    "\n",
    "ax2.set_xlim(times[starttest], times[endtest+Tpredict-1])\n",
    "ax2.set_ylim(-0.6,1.0)\n",
    "ax2.set_title(tit,loc='left')\n",
    "# ax2.set_title(idfig,loc='right',fontsize=10)\n",
    "ax2.legend()\n",
    "\n",
    "if_time = [11]\n",
    "for i in if_time:\n",
    "    hl,=ax3.plot(times[starttest+i:endtest+1+i],corrresult[i,:],label=f'Month {i}')\n",
    "    ax3.plot(times[starttest+i:endtest+1+i],persistence[i,:],color=hl.get_color(), linestyle='dashed')\n",
    "\n",
    "ax3.set_xlim(times[starttest], times[endtest+Tpredict-1])\n",
    "ax3.set_ylim(-0.6,1.0)\n",
    "ax3.set_title(tit,loc='left')\n",
    "# ax3.set_title(idfig,loc='right',fontsize=10)\n",
    "ax3.legend()\n",
    "\n",
    "# Remove the axis and labels for ax4\n",
    "ax4.axis('off')  # This removes the axis lines and ticks\n",
    "ax4.set_xticklabels([])  # This removes the x-axis labels\n",
    "ax4.set_yticklabels([])  # This removes the y-axis labels\n",
    "\n",
    "# Label the experiment\n",
    "# dfig = f\"Predictors={figinfield}, MIN={MIN},MOUT={K},in={TIN},FD={params['FFN_DIM']},H={params['ENC_Heads']},LY={params['enc_dec_layers']}\"\n",
    "delta = 0.0\n",
    "ffcol = {'fontsize':8, 'color':'black'}\n",
    "\n",
    "ax4.text(0, 0.1, f'Input Sequence,  {TIN} month', **ffcol) \n",
    "ax4.text(0, 0.2, f\"Hidden Dimension, {params['FFN_DIM']}\", **ffcol) \n",
    "for ff in INX.keys():\n",
    "    lab = f\"{ff},     eof = {INX[ff]['mr']}\"\n",
    "    ax4.text(0, 0.3+delta, lab, **ffcol) \n",
    "    delta += 0.1\n",
    "    \n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'FIG_{file}_Month.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e51762-8e62-4a9c-a166-63c5e3461d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb71160-03b4-4e4e-a951-0b953542ce34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3051c-d6e7-4a68-a0e3-fcef92e6d3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d8586-5297-467a-b624-4daa4d657213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d7b2e-c34f-424c-bfc6-a06eec0025d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba5adc-f416-4f74-a0e7-562b18054b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
